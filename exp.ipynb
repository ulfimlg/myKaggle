{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-white')\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from skimage.io import imread, imshow\n",
    "from skimage.transform import resize\n",
    "from skimage.morphology import label\n",
    "\n",
    "from keras.models import Model, load_model, save_model\n",
    "from keras.layers import Input, Dropout, BatchNormalization, Activation, Add, ZeroPadding2D, Reshape\n",
    "from keras.layers.core import Lambda\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import array_to_img, img_to_array, load_img\n",
    "\n",
    "import time\n",
    "t_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unet_resnet_v5.model\n",
      "Unet_resnet_v5.csv\n"
     ]
    }
   ],
   "source": [
    "version = 5\n",
    "basic_name = 'Unet_resnet_v5'\n",
    "save_model_name = basic_name + '.model'\n",
    "submission_file = basic_name + '.csv'\n",
    "\n",
    "print(save_model_name)\n",
    "print(submission_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "4a7f9e82027f04b17b56c177a1831abf25abe2c8"
   },
   "outputs": [],
   "source": [
    "img_size_ori = 101\n",
    "img_size_target = 101\n",
    "\n",
    "def upsample(img):# not used\n",
    "    if img_size_ori == img_size_target:\n",
    "        return img\n",
    "    return resize(img, (img_size_target, img_size_target), mode='constant', preserve_range=True)\n",
    "    \n",
    "def downsample(img):# not used\n",
    "    if img_size_ori == img_size_target:\n",
    "        return img\n",
    "    return resize(img, (img_size_ori, img_size_ori), mode='constant', preserve_range=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "419fa6e1ad5218241f08f954453fa3899de8e2e7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading of training/testing ids and depths\n",
    "train_df = pd.read_csv(\"/home/nazimgirach/train.csv\", index_col=\"id\", usecols=[0])\n",
    "depths_df = pd.read_csv(\"/home/nazimgirach/depths.csv\", index_col=\"id\")\n",
    "train_df = train_df.join(depths_df)\n",
    "test_df = depths_df[~depths_df.index.isin(train_df.index)]\n",
    "\n",
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "fc276e696e66de8ab00b68dce1863fa55eb5023d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36ff7574be064907b97ff9ebb954a759",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras_preprocessing/image.py:492: UserWarning: grayscale is deprecated. Please use color_mode = \"grayscale\"\n",
      "  warnings.warn('grayscale is deprecated. Please use '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_df[\"images\"] = [np.array(load_img(\"/home/nazimgirach/train/images/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm_notebook(train_df.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "a2339c6c3b81280839f05269b2314be4541fa5c4"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ab1022d12e04e65b1f9eb5db41f18a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_df[\"masks\"] = [np.array(load_img(\"/home/nazimgirach/train/masks/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm_notebook(train_df.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "cbacf34c7506a867795260a9c615cb4427d62c2d"
   },
   "outputs": [],
   "source": [
    "train_df[\"coverage\"] = train_df.masks.map(np.sum) / pow(img_size_ori, 2)\n",
    "\n",
    "def cov_to_class(val):    \n",
    "    for i in range(0, 11):\n",
    "        if val * 10 <= i :\n",
    "            return i\n",
    "        \n",
    "train_df[\"coverage_class\"] = train_df.coverage.map(cov_to_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "2ea5c45300c08b06c60548dd09831cff38878551"
   },
   "outputs": [],
   "source": [
    "# Create train/validation split stratified by salt coverage\n",
    "ids_train, ids_valid, x_train, x_valid, y_train, y_valid, cov_train, cov_test, depth_train, depth_test = \\\n",
    "train_test_split(train_df.index.values, \n",
    "                 np.array(train_df.images.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1),\n",
    "                 np.array(train_df.masks.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1),\n",
    "                 train_df.coverage.values, \n",
    "                 train_df.z.values, \n",
    "                 test_size=0.2, stratify=train_df.coverage_class, random_state=555)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "e4ee11d11d6a42da26b699cbf9ce5f005de91968"
   },
   "outputs": [],
   "source": [
    "def get_iou_vector(A, B):\n",
    "    batch_size = A.shape[0]\n",
    "    metric = []\n",
    "    for batch in range(batch_size):\n",
    "        t, p = A[batch]>0, B[batch]>0\n",
    "        intersection = np.logical_and(t, p)\n",
    "        union = np.logical_or(t, p)\n",
    "        iou = (np.sum(intersection > 0) + 1e-10 )/ (np.sum(union > 0) + 1e-10)\n",
    "        thresholds = np.arange(0.5, 1, 0.05)\n",
    "        s = []\n",
    "        for thresh in thresholds:\n",
    "            s.append(iou > thresh)\n",
    "        metric.append(np.mean(s))\n",
    "        \n",
    "    return np.mean(metric)\n",
    "\n",
    "def my_iou_metric(label, pred):\n",
    "    return tf.py_func(get_iou_vector, [label, pred>0.5], tf.float64)\n",
    "\n",
    "def my_iou_metric_2(label, pred):\n",
    "    return tf.py_func(get_iou_vector, [label, pred >0], tf.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "72fdacfe6a31500522f7d4c6ba858120b93bc39d"
   },
   "outputs": [],
   "source": [
    "# code download from: https://github.com/bermanmaxim/LovaszSoftmax\n",
    "def lovasz_grad(gt_sorted):\n",
    "    \"\"\"\n",
    "    Computes gradient of the Lovasz extension w.r.t sorted errors\n",
    "    See Alg. 1 in paper\n",
    "    \"\"\"\n",
    "    gts = tf.reduce_sum(gt_sorted)\n",
    "    intersection = gts - tf.cumsum(gt_sorted)\n",
    "    union = gts + tf.cumsum(1. - gt_sorted)\n",
    "    jaccard = 1. - intersection / union\n",
    "    jaccard = tf.concat((jaccard[0:1], jaccard[1:] - jaccard[:-1]), 0)\n",
    "    return jaccard\n",
    "\n",
    "# --------------------------- BINARY LOSSES ---------------------------\n",
    "\n",
    "def lovasz_hinge(logits, labels, per_image=True, ignore=None):\n",
    "    \"\"\"\n",
    "    Binary Lovasz hinge loss\n",
    "      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n",
    "      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n",
    "      per_image: compute the loss per image instead of per batch\n",
    "      ignore: void class id\n",
    "    \"\"\"\n",
    "    if per_image:\n",
    "        def treat_image(log_lab):\n",
    "            log, lab = log_lab\n",
    "            log, lab = tf.expand_dims(log, 0), tf.expand_dims(lab, 0)\n",
    "            log, lab = flatten_binary_scores(log, lab, ignore)\n",
    "            return lovasz_hinge_flat(log, lab)\n",
    "        losses = tf.map_fn(treat_image, (logits, labels), dtype=tf.float32)\n",
    "        loss = tf.reduce_mean(losses)\n",
    "    else:\n",
    "        loss = lovasz_hinge_flat(*flatten_binary_scores(logits, labels, ignore))\n",
    "    return loss\n",
    "\n",
    "def lovasz_hinge_flat(logits, labels):\n",
    "    \"\"\"\n",
    "    Binary Lovasz hinge loss\n",
    "      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n",
    "      labels: [P] Tensor, binary ground truth labels (0 or 1)\n",
    "      ignore: label to ignore\n",
    "    \"\"\"\n",
    "\n",
    "    def compute_loss():\n",
    "        labelsf = tf.cast(labels, logits.dtype)\n",
    "        signs = 2. * labelsf - 1.\n",
    "        errors = 1. - logits * tf.stop_gradient(signs)\n",
    "        errors_sorted, perm = tf.nn.top_k(errors, k=tf.shape(errors)[0], name=\"descending_sort\")\n",
    "        gt_sorted = tf.gather(labelsf, perm)\n",
    "        grad = lovasz_grad(gt_sorted)\n",
    "        loss = tf.tensordot(tf.nn.elu(errors_sorted), tf.stop_gradient(grad), 1, name=\"loss_non_void\")\n",
    "        return loss\n",
    "\n",
    "    # deal with the void prediction case (only void pixels)\n",
    "    loss = tf.cond(tf.equal(tf.shape(logits)[0], 0),\n",
    "                   lambda: tf.reduce_sum(logits) * 0.,\n",
    "                   compute_loss,\n",
    "                   strict=True,\n",
    "                   name=\"loss\"\n",
    "                   )\n",
    "    return loss\n",
    "\n",
    "\n",
    "def flatten_binary_scores(scores, labels, ignore=None):\n",
    "    \"\"\"\n",
    "    Flattens predictions in the batch (binary case)\n",
    "    Remove labels equal to 'ignore'\n",
    "    \"\"\"\n",
    "    scores = tf.reshape(scores, (-1,))\n",
    "    labels = tf.reshape(labels, (-1,))\n",
    "    if ignore is None:\n",
    "        return scores, labels\n",
    "    valid = tf.not_equal(labels, ignore)\n",
    "    vscores = tf.boolean_mask(scores, valid, name='valid_scores')\n",
    "    vlabels = tf.boolean_mask(labels, valid, name='valid_labels')\n",
    "    return vscores, vlabels\n",
    "\n",
    "def lovasz_loss(y_true, y_pred):\n",
    "    y_true, y_pred = K.cast(K.squeeze(y_true, -1), 'int32'), K.cast(K.squeeze(y_pred, -1), 'float32')\n",
    "    #logits = K.log(y_pred / (1. - y_pred))\n",
    "    logits = y_pred #Jiaxin\n",
    "    loss = lovasz_hinge(logits, y_true, per_image = True, ignore = None)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "3a4ffe206d6ea50da3d8ccb8ec8d219a2e6cca94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6400, 101, 101, 1)\n",
      "(800, 101, 101, 1)\n"
     ]
    }
   ],
   "source": [
    "#Data augmentation\n",
    "x_train = np.append(x_train, [np.fliplr(x) for x in x_train], axis=0)\n",
    "y_train = np.append(y_train, [np.fliplr(x) for x in y_train], axis=0)\n",
    "print(x_train.shape)\n",
    "print(y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "37edb0d94b2c62a5b13f9504b237c1b4f0e16852"
   },
   "outputs": [],
   "source": [
    "def BatchActivate(x):\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "def convolution_block(x, filters, size, strides=(1,1), padding='same', activation=True):\n",
    "    x = Conv2D(filters, size, strides=strides, padding=padding)(x)\n",
    "    if activation==True: x = BatchActivate(x)\n",
    "    return x\n",
    "\n",
    "def residual_block(blockInput, num_filters=16, batch_activate=False):\n",
    "    x = BatchActivate(blockInput)\n",
    "    x = convolution_block(x, num_filters, (3,3))\n",
    "    x = convolution_block(x, num_filters, (3,3), activation=False)\n",
    "    x = Add()([x, blockInput])\n",
    "    if batch_activate: x = BatchActivate(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import GlobalAveragePooling2D, Reshape, Dense, multiply, Permute\n",
    "from keras import backend as K\n",
    "def squeeze_excite_block(input, filters, ratio=16):\n",
    "    init = input\n",
    "    filters = filters\n",
    "    se_shape = (1, 1, filters)\n",
    "\n",
    "    se = GlobalAveragePooling2D()(init)\n",
    "    se = Reshape(se_shape)(se)\n",
    "    se = Dense(filters // ratio, activation='relu', kernel_initializer='he_normal', use_bias=False)(se)\n",
    "    se = Dense(filters, activation='sigmoid', kernel_initializer='he_normal', use_bias=False)(se)\n",
    "\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        se = Permute((3, 1, 2))(se)\n",
    "\n",
    "    x = multiply([init, se])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "b30682ea376415e64702ee8c7a4d0f21d2e1d9f7"
   },
   "outputs": [],
   "source": [
    "def myres(x, start_neurons, up = False):\n",
    "    conv33 = Conv2D(start_neurons, (1,1), strides=(1,1), activation=None, padding='same')(x)\n",
    "    conv33 = BatchNormalization()(conv33)\n",
    "    conv33 = Activation('relu')(conv33)\n",
    "    conv33 = Conv2D(start_neurons, (3,3), strides=(1,1), padding='same')(conv33)\n",
    "    conv33 = BatchNormalization()(conv33)\n",
    "    conv33 = Activation('relu')(conv33)\n",
    "    if up:\n",
    "        x = Conv2D(start_neurons, (1,1), strides=(1,1), activation=None, padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x33 = Add()([conv33, x])\n",
    "        return x33\n",
    "    conv33 = Conv2D(start_neurons*4, (1,1), strides=(1,1), activation=None, padding='same')(conv33)\n",
    "    #conv33 = BatchNormalization()(conv33)\n",
    "    conv33 = squeeze_excite_block(conv33,start_neurons*4)\n",
    "    x33 = Add()([conv33, x])\n",
    "    x33 = Activation('relu')(x33)\n",
    "    return x33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "b4906771ce2f038a3c734d031bbdb369f5d69680"
   },
   "outputs": [],
   "source": [
    "# Build Model\n",
    "def build_resnet(input_layer, start_neurons, DropoutRatio=0.5):\n",
    "    #101\n",
    "    #Stage=0 Block=0, stride=(1,1) and Add()\n",
    "    conv01 = Conv2D(start_neurons, (1,1), strides=(1,1), activation=None, padding='same')(input_layer)\n",
    "    conv01 = BatchNormalization()(conv01)\n",
    "    conv01 = Activation('relu')(conv01)\n",
    "    conv01 = Conv2D(start_neurons, (3,3), strides=(1,1), padding='same')(conv01)\n",
    "    conv01 = BatchNormalization()(conv01)\n",
    "    conv01 = Activation('relu')(conv01)\n",
    "    conv01 = Conv2D(start_neurons*4, (1,1), strides=(1,1), activation=None, padding='same')(conv01)\n",
    "    conv01a = squeeze_excite_block(conv01,start_neurons*4)#BatchNormalization()(conv01)\n",
    "    conv01b = Conv2D(start_neurons*4, (1,1), strides=(1,1), activation=None, padding='same')(input_layer)\n",
    "    conv01b = BatchNormalization()(conv01b)\n",
    "    x0 = Add()([conv01a, conv01b])\n",
    "    x0 = Activation('relu')(x0)\n",
    "    for i in range(1,3):#3\n",
    "        x0 = myres(x0, start_neurons)\n",
    "    #Stage=0 Block=1, stride=(1,1)\n",
    "    mp = MaxPooling2D((2,2))(x0)\n",
    "############################################################3\n",
    "    start_neurons=start_neurons*2\n",
    "    #50\n",
    "    #Stage=1 Block=0, stride=(2,2) and Add()\n",
    "    conv02 = Conv2D(start_neurons, (1,1), strides=(1,1), activation=None, padding='same')(mp)\n",
    "    conv02 = BatchNormalization()(conv02)\n",
    "    conv02 = Activation('relu')(conv02)\n",
    "    conv02 = Conv2D(start_neurons, (3,3), strides=(1,1), padding='same')(conv02)\n",
    "    conv02 = BatchNormalization()(conv02)\n",
    "    conv02 = Activation('relu')(conv02)\n",
    "    conv02 = Conv2D(start_neurons*4, (1,1), strides=(1,1), activation=None, padding='same')(conv02)\n",
    "    conv02a = squeeze_excite_block(conv02,start_neurons*4)#conv02a = BatchNormalization()(conv02)\n",
    "    conv02b = Conv2D(start_neurons*4, (1,1), strides=(1,1), activation=None, padding='same')(mp)\n",
    "    conv02b = BatchNormalization()(conv02b)\n",
    "    x02 = Add()([conv02a, conv02b])\n",
    "    x02 = Activation('relu')(x02)\n",
    "    for i in range(1,5):#5\n",
    "        x02 = myres(x02, start_neurons)\n",
    "    x1 = x02\n",
    "##########################################################\n",
    "    start_neurons=start_neurons*2\n",
    "    #25\n",
    "    #Stage=2 Block=0, stride=(2,2) \n",
    "    conv03 = Conv2D(start_neurons, (1,1), strides=(2,2), activation=None, padding='same')(x1)\n",
    "    conv03 = BatchNormalization()(conv03)\n",
    "    conv03 = Activation('relu')(conv03)\n",
    "    conv03 = Conv2D(start_neurons, (3,3), strides=(1,1), padding='same')(conv03)\n",
    "    conv03 = BatchNormalization()(conv03)\n",
    "    conv03 = Activation('relu')(conv03)\n",
    "    conv03 = Conv2D(start_neurons*4, (1,1), strides=(1,1), activation=None, padding='same')(conv03)\n",
    "    conv03a = squeeze_excite_block(conv03,start_neurons*4)#conv03a = BatchNormalization()(conv03)\n",
    "    conv03b = Conv2D(start_neurons*4, (1,1), strides=(2,2), activation=None, padding='valid')(x1)\n",
    "    conv03b = BatchNormalization()(conv03b)\n",
    "    x03 = Add()([conv03a, conv03b])\n",
    "    x03 = Activation('relu')(x03)\n",
    "    x = x03\n",
    "    #Stage=2 Block=1, stride=(1,1)\n",
    "    for i in range(1,8):#8\n",
    "        x = myres(x, start_neurons)\n",
    "        \n",
    "    x2 = x\n",
    "    ############################################\n",
    "    start_neurons=start_neurons*2\n",
    "    #12\n",
    "    #Stage=3 Block=0, stride=(2,2) and Add()\n",
    "    mp3= MaxPooling2D((2,2))(x2)\n",
    "    conv04 = Conv2D(start_neurons, (1,1), strides=(1,1), activation=None, padding='same')(mp3)\n",
    "    conv04 = BatchNormalization()(conv04)\n",
    "    conv04 = Activation('relu')(conv04)\n",
    "    conv04 = Conv2D(start_neurons, (3,3), strides=(1,1), padding='same')(conv04)\n",
    "    conv04 = BatchNormalization()(conv04)\n",
    "    conv04 = Activation('relu')(conv04)\n",
    "    conv04 = Conv2D(start_neurons*4, (1,1), strides=(1,1), activation=None, padding='same')(conv04)\n",
    "    conv04a = squeeze_excite_block(conv04,start_neurons*4)#conv04a = BatchNormalization()(conv04)\n",
    "    conv04b = Conv2D(start_neurons*4, (1,1), strides=(1,1), activation=None, padding='valid')(mp3)\n",
    "    conv04b = BatchNormalization()(conv04b)\n",
    "    x04 = Add()([conv04a, conv04b])\n",
    "    x04 = Activation('relu')(x04)\n",
    "    #Stage=3 Block=1, stride=(1,1)\n",
    "    x = x04\n",
    "    for i in range(1,36):#36\n",
    "        x = myres(x, start_neurons)\n",
    "    x3 = x\n",
    "    ###################################################\n",
    "    start_neurons=start_neurons*2\n",
    "    #6\n",
    "    #Stage=4 Block=0, stride=(2,2) and Add()\n",
    "    conv05 = Conv2D(start_neurons, (1,1), strides=(2,2), activation=None, padding='same')(x3)\n",
    "    conv05 = BatchNormalization()(conv05)\n",
    "    conv05 = Activation('relu')(conv05)\n",
    "    conv05 = Conv2D(start_neurons, (3,3), strides=(1,1), padding='same')(conv05)\n",
    "    conv05 = BatchNormalization()(conv05)\n",
    "    conv05 = Activation('relu')(conv05)\n",
    "    conv05 = Conv2D(start_neurons*4, (1,1), strides=(1,1), activation=None, padding='same')(conv05)\n",
    "    conv05a = squeeze_excite_block(conv05,start_neurons*4)#conv05a = BatchNormalization()(conv05)\n",
    "    conv05b = Conv2D(start_neurons*4, (1,1), strides=(2,2), activation=None, padding='valid')(x04)\n",
    "    conv05b = BatchNormalization()(conv05b)\n",
    "    x05 = Add()([conv05a, conv05b])\n",
    "    x05 = Activation('relu')(x05)\n",
    "    x = x05\n",
    "    for i in range(1,5):#5\n",
    "        x = myres(x, start_neurons)\n",
    "    x4 = x\n",
    "    \n",
    "    ####################################################\n",
    "    # 6 -> 13, 512\n",
    "    \n",
    "    deconv4 = Conv2DTranspose(start_neurons, (3,3), strides=(2,2), padding='same')(x4)\n",
    "    uconv4 = concatenate([deconv4, x3])\n",
    "    #uconv4 = Dropout(DropoutRatio)(uconv4)\n",
    "    \n",
    "    \n",
    "    uconv4 = Conv2D(start_neurons*2,(3,3), activation=None, padding='same')(uconv4)\n",
    "    uconv4 = BatchNormalization()(uconv4)\n",
    "    uconv4 = Activation('relu')(uconv4)\n",
    "    for i in range(1,4):#4\n",
    "        uconv4 = myres(uconv4, start_neurons*2, True)\n",
    "    ######################################################\n",
    "    # 12 -> 25\n",
    "    \n",
    "    deconv3 = Conv2DTranspose(start_neurons, (3,3), strides=(2,2), padding='valid')(uconv4)\n",
    "    uconv3 = concatenate([deconv3, x2])\n",
    "    #uconv3 = Dropout(DropoutRatio)(uconv3)\n",
    "    \n",
    "    uconv3 = Conv2D(start_neurons, (3,3), activation=None, padding='same')(uconv3)\n",
    "    uconv3 = BatchNormalization()(uconv3)\n",
    "    uconv3 = Activation('relu')(uconv3)\n",
    "    for i in range(1,2):#4\n",
    "        uconv3 = myres(uconv3, start_neurons, True)\n",
    "    #######################################################\n",
    "    # 25 -> 50\n",
    "    deconv2 = Conv2DTranspose(start_neurons//2, (3,3), strides=(2,2), padding='same')(uconv3)\n",
    "    uconv2 = concatenate([deconv2, x1])\n",
    "    uconv2 = Dropout(DropoutRatio)(uconv2)\n",
    "    \n",
    "    uconv2 = Conv2D(start_neurons//2, (3,3), activation=None, padding='same')(uconv2)\n",
    "    uconv2 = BatchNormalization()(uconv2)\n",
    "    uconv2 = Activation('relu')(uconv2)\n",
    "    for i in range(1,2):#4\n",
    "        uconv2 = myres(uconv2, start_neurons//2, True)\n",
    "    #########################################################\n",
    "    #conv_50 = Conv2DTranspose(start_neurons//4, (3,3), strides=(2,2), padding='valid')(x1)\n",
    "    conv_50 = Conv2D(start_neurons//2, kernel_size=1, strides=1, padding='same', kernel_initializer=\"he_normal\")(x1)\n",
    "    \n",
    "    x2 = Conv2D(start_neurons//2, kernel_size=1, strides=1, padding='same', kernel_initializer=\"he_normal\")(x2)\n",
    "    conv_25 = Conv2DTranspose(start_neurons//2, (3,3), strides=(2,2), padding='same')(x2)\n",
    "    conv_25 = Conv2D(start_neurons//2, kernel_size=1, strides=1, padding='same', kernel_initializer=\"he_normal\")(conv_25)\n",
    "    \n",
    "    x3 = Conv2D(start_neurons//2, kernel_size=1, strides=1, padding='same', kernel_initializer=\"he_normal\")(x3)\n",
    "    conv_12 = Conv2DTranspose(start_neurons//2, (3,3), strides=(2,2), padding='valid')(x3)\n",
    "    conv_12 = Conv2DTranspose(start_neurons//2, (3,3), strides=(2,2), padding='same')(conv_12)\n",
    "    conv_12 = Conv2D(start_neurons//2, kernel_size=1, strides=1, padding='same', kernel_initializer=\"he_normal\")(conv_12)\n",
    "\n",
    "    x4 = Conv2D(start_neurons//2, kernel_size=1, strides=1, padding='same', kernel_initializer=\"he_normal\")(x4)\n",
    "    conv_60 = Conv2DTranspose(start_neurons//2, (3,3), strides=(2,2), padding='same')(x4)\n",
    "    conv_61 = Conv2DTranspose(start_neurons//2, (3,3), strides=(2,2), padding='valid')(conv_60)\n",
    "    conv_6 = Conv2DTranspose(start_neurons//2, (3,3), strides=(2,2), padding='same')(conv_61)\n",
    "    conv_6 = Conv2D(start_neurons//2, kernel_size=1, strides=1, padding='same', kernel_initializer=\"he_normal\")(conv_6)\n",
    "    #conv_6 = Conv2DTranspose(start_neurons//4, (3,3), strides=(2,2), padding='valid')(conv_62)\n",
    "    #print(conv_6._keras_shape)\n",
    "    #########################################################\n",
    "    #########################################################\n",
    "    # 50 -> 101\n",
    "    con = concatenate([conv_50, conv_25, conv_12, conv_6])\n",
    "    deconv1 = Conv2DTranspose(start_neurons//4, (3,3), strides=(2,2), padding='valid')(uconv2)\n",
    "    uconv1 = concatenate([deconv1, x0])\n",
    "    uconv1 = Dropout(DropoutRatio)(uconv1)\n",
    "    \n",
    "    uconv1 = Conv2D(start_neurons//4, (3,3), activation=None, padding='same')(uconv1)\n",
    "    uconv1 = residual_block(uconv1, start_neurons//4)\n",
    "    uconv1 = residual_block(uconv1, start_neurons//4, True)\n",
    "    \n",
    "    output_layer_noActi = Conv2D(1, (1,1), padding='same', activation=None)(uconv1)\n",
    "    output_layer = Activation('sigmoid')(output_layer_noActi)\n",
    "    \n",
    "    return output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2c367f084c271d21f8ec61e36568d7e097ed5179"
   },
   "outputs": [],
   "source": [
    "# model\n",
    "from keras.utils.training_utils import multi_gpu_model\n",
    "G = 1\n",
    "if G <= 1:\n",
    "    print(\"[INFO] training with 1 GPU...\")\n",
    "    input_layer = Input((img_size_target, img_size_target, 1))\n",
    "    output_layer = build_resnet(input_layer, 32,0.5)\n",
    "    model = Model(input_layer, output_layer)\n",
    "# otherwise, we are compiling using multiple GPUs\n",
    "else:\n",
    "    print(\"[INFO] training with {} GPUs...\".format(G))\n",
    "\n",
    "    # we'll store a copy of the model on *every* GPU and then combine\n",
    "    # the results from the gradient updates on the CPU\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        # initialize the model\n",
    "        input_layer = Input((img_size_target, img_size_target, 1))\n",
    "        output_layer = build_resnet(input_layer, 16,0.5)\n",
    "        modell = Model(input_layer, output_layer)\n",
    "    # make the model parallel\n",
    "    model = multi_gpu_model(modell, gpus=G)\n",
    "\n",
    "    \n",
    "# model\n",
    "c = optimizers.adam(lr = 0.005)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=c, metrics=[my_iou_metric])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bd402f81e3d8e0aad30b7c63836cb9d488105cef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6400 samples, validate on 800 samples\n",
      "Epoch 1/2\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_my_iou_metric', mode = 'max',patience=20, verbose=1)\n",
    "model_checkpoint = ModelCheckpoint(save_model_name, monitor='my_iou_metric', mode='max',\n",
    "                                   save_best_only=True, verbose=1)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_my_iou_metric', mode='max', factor=0.5, patience=5,\n",
    "                              min_lr=0.00001, verbose=1)\n",
    "epochs = 2\n",
    "batch_size = 22\n",
    "\n",
    "t_model1_start = time.time()\n",
    "history = model.fit(x_train, y_train,\n",
    "                     validation_data = [x_valid, y_valid], \n",
    "                     epochs = epochs, \n",
    "                     batch_size = batch_size, \n",
    "                     callbacks = [early_stopping, reduce_lr],\n",
    "                     verbose = 1)\n",
    "t_model1_end = time.time()\n",
    "print(\"Run time = \"+str((t_model1_end-t_model1_start)/3600)+\" hours\")\n",
    "modell.save('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "_uuid": "52ba1ba40cac42145f499cff252eb73feff397d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training with 2 GPUs...\n"
     ]
    }
   ],
   "source": [
    "#model1 = load_model(save_model_name, custom_objects={'my_iou_metric':my_iou_metric})\n",
    "# remove activation layer and use lovasz loss\n",
    "from keras.utils.training_utils import multi_gpu_model\n",
    "save_model_name='my_model.h5'\n",
    "G = 2\n",
    "if G <= 1:\n",
    "    print(\"[INFO] training with 1 GPU...\")\n",
    "    input_layer = Input((img_size_target, img_size_target, 1))\n",
    "    output_layer = build_resnet(input_layer, 16,0.5)\n",
    "    model = Model(input_layer, output_layer)\n",
    "# otherwise, we are compiling using multiple GPUs\n",
    "else:\n",
    "    print(\"[INFO] training with {} GPUs...\".format(G))\n",
    "\n",
    "    # we'll store a copy of the model on *every* GPU and then combine\n",
    "    # the results from the gradient updates on the CPU\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        # initialize the model\n",
    "        model1 = load_model(save_model_name, custom_objects={'my_iou_metric':my_iou_metric})\n",
    "        # remove activation layer and use lovasz loss\n",
    "        input_x = model1.layers[0].input\n",
    "\n",
    "        output_layer = model1.layers[-1].input\n",
    "        modell = Model(input_x, output_layer)\n",
    "        input_layer = Input((img_size_target, img_size_target, 1))\n",
    "        output_layer = build_resnet(input_layer, 16,0.5)\n",
    "        modell = Model(input_layer, output_layer)\n",
    "    # make the model parallel\n",
    "    model = multi_gpu_model(modell, gpus=G)\n",
    "c = optimizers.adam(lr=0.001)\n",
    "\n",
    "model.compile(loss=[\"binary_crossentropy\",lovasz_loss],loss_weights=[0.6,0.4], optimizer=c, metrics=[my_iou_metric_2])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ec1e49fd425a02ab12dea6b99cf39de45c80199f"
   },
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_my_iou_metric_2', mode = 'max',patience=30, verbose=1)\n",
    "model_checkpoint = ModelCheckpoint(save_model_name,monitor='val_my_iou_metric_2', \n",
    "                                   mode = 'max', save_best_only=True, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_my_iou_metric_2', mode = 'max',factor=0.5, patience=5, \n",
    "                              min_lr=0.00005, verbose=1)\n",
    "epochs = 2\n",
    "batch_size = 32\n",
    "\n",
    "t_model2_start = time.time()\n",
    "history = model.fit(x_train, y_train,\n",
    "                    validation_data=[x_valid, y_valid], \n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=[reduce_lr,early_stopping], \n",
    "                    verbose=1)\n",
    "t_model2_end = time.time()\n",
    "print(\"Run time = \"+str((t_model2_end-t_model2_start)/3600)+\" hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "modell.save(save_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "068c89f56e9e7eb1d7ecf12912996426ad87f2a6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "model = load_model(save_model_name,custom_objects={'my_iou_metric_2': my_iou_metric_2,\n",
    "                                                   'lovasz_loss': lovasz_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=lovasz_loss, optimizer=c, metrics=[my_iou_metric_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_uuid": "c33f2cdf50d3011f704ac934e49602bd3167c8cb"
   },
   "outputs": [],
   "source": [
    "def predict_result(model,x_test,img_size_target): # predict both orginal and reflect x\n",
    "    x_test_reflect =  np.array([np.fliplr(x) for x in x_test])\n",
    "    preds_test = model.predict(x_test).reshape(-1, img_size_target, img_size_target)\n",
    "    preds_test2_refect = model.predict(x_test_reflect).reshape(-1, img_size_target, img_size_target)\n",
    "    preds_test += np.array([ np.fliplr(x) for x in preds_test2_refect] )\n",
    "    return preds_test/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_uuid": "ec38f74adccf2d265cac160311c0998fefc76ec6"
   },
   "outputs": [],
   "source": [
    "preds_valid = predict_result(model,x_valid,img_size_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_uuid": "6794a936dd110f0d4ecc23206c53af7cd0f4a587"
   },
   "outputs": [],
   "source": [
    "#Score the model and do a threshold optimization by the best IoU.\n",
    "\n",
    "# src: https://www.kaggle.com/aglotero/another-iou-metric\n",
    "def iou_metric(y_true_in, y_pred_in, print_table=False):\n",
    "    labels = y_true_in\n",
    "    y_pred = y_pred_in\n",
    "\n",
    "\n",
    "    true_objects = 2\n",
    "    pred_objects = 2\n",
    "\n",
    "    #  if all zeros, original code  generate wrong  bins [-0.5 0 0.5],\n",
    "    temp1 = np.histogram2d(labels.flatten(), y_pred.flatten(), bins=([0,0.5,1], [0,0.5, 1]))\n",
    "#     temp1 = np.histogram2d(labels.flatten(), y_pred.flatten(), bins=(true_objects, pred_objects))\n",
    "    #print(temp1)\n",
    "    intersection = temp1[0]\n",
    "    #print(\"temp2 = \",temp1[1])\n",
    "    #print(intersection.shape)\n",
    "   # print(intersection)\n",
    "    # Compute areas (needed for finding the union between all objects)\n",
    "    #print(np.histogram(labels, bins = true_objects))\n",
    "    area_true = np.histogram(labels,bins=[0,0.5,1])[0]\n",
    "    #print(\"area_true = \",area_true)\n",
    "    area_pred = np.histogram(y_pred, bins=[0,0.5,1])[0]\n",
    "    area_true = np.expand_dims(area_true, -1)\n",
    "    area_pred = np.expand_dims(area_pred, 0)\n",
    "\n",
    "    # Compute union\n",
    "    union = area_true + area_pred - intersection\n",
    "  \n",
    "    # Exclude background from the analysis\n",
    "    intersection = intersection[1:,1:]\n",
    "    intersection[intersection == 0] = 1e-9\n",
    "    \n",
    "    union = union[1:,1:]\n",
    "    union[union == 0] = 1e-9\n",
    "\n",
    "    # Compute the intersection over union\n",
    "    iou = intersection / union\n",
    "    # Precision helper function\n",
    "    def precision_at(threshold, iou):\n",
    "        matches = iou > threshold\n",
    "        true_positives = np.sum(matches, axis=1) == 1   # Correct objects\n",
    "        false_positives = np.sum(matches, axis=0) == 0  # Missed objects\n",
    "        false_negatives = np.sum(matches, axis=1) == 0  # Extra objects\n",
    "        tp, fp, fn = np.sum(true_positives), np.sum(false_positives), np.sum(false_negatives)\n",
    "        return tp, fp, fn\n",
    "\n",
    "    # Loop over IoU thresholds\n",
    "    prec = []\n",
    "    if print_table:\n",
    "        print(\"Thresh\\tTP\\tFP\\tFN\\tPrec.\")\n",
    "    for t in np.arange(0.5, 1.0, 0.05):\n",
    "        tp, fp, fn = precision_at(t, iou)\n",
    "        if (tp + fp + fn) > 0:\n",
    "            p = tp / (tp + fp + fn)\n",
    "        else:\n",
    "            p = 0\n",
    "        if print_table:\n",
    "            print(\"{:1.3f}\\t{}\\t{}\\t{}\\t{:1.3f}\".format(t, tp, fp, fn, p))\n",
    "        prec.append(p)\n",
    "        \n",
    "    if print_table:\n",
    "        print(\"AP\\t-\\t-\\t-\\t{:1.3f}\".format(np.mean(prec)))\n",
    "    return np.mean(prec)\n",
    "\n",
    "def iou_metric_batch(y_true_in, y_pred_in):\n",
    "    batch_size = y_true_in.shape[0]\n",
    "    metric = []\n",
    "    for batch in range(batch_size):\n",
    "        value = iou_metric(y_true_in[batch], y_pred_in[batch])\n",
    "        metric.append(value)\n",
    "    return np.mean(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_uuid": "6d96072932f985336021cd115cb6bb13058618a8"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80f5bd3d04f34446bc8a208afa24522d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=31), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[0.132625 0.132625 0.132625 0.132625 0.132625 0.132625 0.132625 0.132625\n",
      " 0.132625 0.132625 0.132625 0.132625 0.132625 0.132625 0.132625 0.132625\n",
      " 0.39     0.39     0.39     0.39     0.39     0.39     0.39     0.39\n",
      " 0.39     0.39     0.39     0.39     0.39     0.39     0.39    ]\n"
     ]
    }
   ],
   "source": [
    "## Scoring for last model, choose threshold by validation data \n",
    "thresholds_ori = np.linspace(0.3, 0.7, 31)\n",
    "# Reverse sigmoid function: Use code below because the  sigmoid activation was removed\n",
    "thresholds = np.log(thresholds_ori/(1-thresholds_ori)) \n",
    "\n",
    "# ious = np.array([get_iou_vector(y_valid, preds_valid > threshold) for threshold in tqdm_notebook(thresholds)])\n",
    "# print(ious)\n",
    "ious = np.array([iou_metric_batch(y_valid, preds_valid > threshold) for threshold in tqdm_notebook(thresholds)])\n",
    "print(ious)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_uuid": "ce197d87cf44fef681025160cba8e4081d15ed50"
   },
   "outputs": [],
   "source": [
    "threshold_best_index = np.argmax(ious)\n",
    "iou_best = ious[threshold_best_index]\n",
    "threshold_best = thresholds[threshold_best_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_uuid": "2633e0c50ddb705021a35441c6a19fae9312603e"
   },
   "outputs": [],
   "source": [
    "def rle_encode(im):\n",
    "    '''\n",
    "    im: numpy array, 1-mask, 0-background\n",
    "    Returns run length as string\n",
    "    '''\n",
    "    pixels = im.flatten(order='F')\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_uuid": "3c29b25a37ceb7476bc0264ea3a97dc705e07607"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed45a3f2eb344d39b453563de8b2463e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=18000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "x_test = np.array([(np.array(load_img(\"/home/nazimgirach/test/images/{}.png\".format(idx), grayscale = True))) / 255 for idx in tqdm_notebook(test_df.index)]).reshape(-1, img_size_target, img_size_target, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e129e7c3e525b884322570ed37da825cd89db290"
   },
   "outputs": [],
   "source": [
    "preds_test = predict_result(model,x_test,img_size_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "aac939d180585abab6dd497a45f81e3df67b25a8"
   },
   "outputs": [],
   "source": [
    "pred_dict = {idx: rle_encode(np.round(downsample(preds_test[i]) > threshold_best)) for i, idx in enumerate(tqdm_notebook(test_df.index.values))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2d6ef314f368bf9923285007e7098c068af049d9"
   },
   "outputs": [],
   "source": [
    "sub = pd.DataFrame.from_dict(pred_dict,orient='index')\n",
    "sub.index.names = ['id']\n",
    "sub.columns = ['rle_mask']\n",
    "sub.to_csv(submission_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ae636758e7abdc7fb87768cbfb2c6f18f8c1975f"
   },
   "outputs": [],
   "source": [
    "t_finish = time.time()\n",
    "print(\"Kernel run time = \"+str((t_finish-t_start)/3600)+\" hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "76d50b9d2159775ae3a84336ef0c01cbcf39a18e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d352221498b0561048c2dea2062aa885ad70829e"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
