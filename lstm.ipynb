{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "522ce330-d4f9-4fbe-9684-4b65fd684cca",
    "_uuid": "174484daa5f084ce4970f5048d3e05d2c4429787"
   },
   "source": [
    "# Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d8ccded02a5f2c4a9d9ee2f7688114bcd2e1f11a"
   },
   "source": [
    "\n",
    "This notebook is a combination of the data generator from Beluga's notebook: https://www.kaggle.com/gaborfodor/greyscale-mobilenet-lb-0-892 and largely based on Kevin Mader's LSTM code, with modifications in the network architecture https://www.kaggle.com/kmader/quickdraw-baseline-lstm-reading-and-submission. \n",
    "\n",
    "I am grateful for their contributions and can take little credit for this notebook. Running this notebook should achieve 0.823 on the LB. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "8b08fbab2000a563b388f126eac74362641e497c"
   },
   "outputs": [],
   "source": [
    "debug = True\n",
    "if debug: \n",
    "    STEPS = 200\n",
    "    val_steps = 10\n",
    "else:\n",
    "    STEPS = 800\n",
    "    val_steps = 100\n",
    "    \n",
    "STROKE_COUNT = 100\n",
    "EPOCHS = 15\n",
    "batchsize = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c327918e9d40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "from keras.metrics import top_k_categorical_accuracy\n",
    "def top_3_accuracy(x,y): return top_k_categorical_accuracy(x,y, 3)\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "from glob import glob\n",
    "import gc\n",
    "gc.enable()\n",
    "def get_available_gpus():\n",
    "    from tensorflow.python.client import device_lib\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "a7a5e3b6503b24a34e60062db6248730a4e96adf"
   },
   "outputs": [],
   "source": [
    "def preds2catids(predictions):\n",
    "    return pd.DataFrame(np.argsort(-predictions, axis=1)[:, :3], columns=['a', 'b', 'c'])\n",
    "\n",
    "def top_3_accuracy(y_true, y_pred):\n",
    "    return top_k_categorical_accuracy(y_true, y_pred, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "7acacf8e960084782425ef1a1a3fd532a240ad48"
   },
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "def _stack_it(raw_strokes):\n",
    "    \"\"\"preprocess the string and make \n",
    "    a standard Nx3 stroke vector\"\"\"\n",
    "    stroke_vec = literal_eval(raw_strokes) # string->list\n",
    "    # unwrap the list\n",
    "    in_strokes = [(xi,yi,i)  \n",
    "     for i,(x,y) in enumerate(stroke_vec) \n",
    "     for xi,yi in zip(x,y)]\n",
    "    c_strokes = np.stack(in_strokes)\n",
    "    # replace stroke id with 1 for continue, 2 for new\n",
    "    c_strokes[:,2] = [1]+np.diff(c_strokes[:,2]).tolist()\n",
    "    c_strokes[:,2] += 1 # since 0 is no stroke\n",
    "    # pad the strokes with zeros\n",
    "    return pad_sequences(c_strokes.swapaxes(0, 1), \n",
    "                         maxlen=STROKE_COUNT, \n",
    "                         padding='post').swapaxes(0, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "c38d52616f2fbec1c6cd6162b36627314c303b0a"
   },
   "outputs": [],
   "source": [
    "DP_DIR = '../input/shuffle-csv-50k'\n",
    "INPUT_DIR = '../input/quickdraw-doodle-recognition'\n",
    "BASE_SIZE = 256\n",
    "NCSVS = 100\n",
    "NCATS = 340\n",
    "np.random.seed(seed=1987)\n",
    "tf.set_random_seed(seed=1987)\n",
    "\n",
    "def f2cat(filename: str) -> str:\n",
    "    return filename.split('.')[0]\n",
    "\n",
    "def list_all_categories():\n",
    "    files = os.listdir(os.path.join(INPUT_DIR, 'train_simplified'))\n",
    "    return sorted([f2cat(f) for f in files], key=str.lower)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "1f5c302cba57d61320a39075d00ddc1aa08f76e5"
   },
   "outputs": [],
   "source": [
    "def image_generator_xd( batchsize, ks):\n",
    "    while True:\n",
    "        for k in np.random.permutation(ks):\n",
    "            filename = os.path.join(DP_DIR, 'train_k{}.csv.gz.gz'.format(k))\n",
    "            for df in pd.read_csv(filename, chunksize=batchsize):\n",
    "                \n",
    "                df['drawing'] = df['drawing'].map(_stack_it)\n",
    "                x2 = np.stack(df['drawing'], 0)\n",
    "                y = keras.utils.to_categorical(df.y, num_classes=NCATS)\n",
    "                yield x2, y\n",
    "\n",
    "def df_to_image_array_xd(df):\n",
    "    df['drawing'] = df['drawing'].map(_stack_it)\n",
    "    x2 = np.stack(df['drawing'], 0)\n",
    "    return x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train_datagen = image_generator_xd(batchsize=batchsize, ks=range(NCSVS - 2))\n",
    "val_datagen = image_generator_xd(batchsize=batchsize, ks=range(NCSVS - 2, NCSVS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6d29237e-ece3-4dfd-9095-475296f4a608",
    "_uuid": "8bae16a4973a215861fbb536a602c4f5abf3b4bf"
   },
   "source": [
    "### Stroke-based Classification\n",
    "Here we use the stroke information to train a model and see if the strokes give us a better idea of what the shape could be. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5e1d5bba-0fb4-432c-bd0b-ad69be0ef9ac",
    "_uuid": "b4a087a17798c2ec8eb520bc916bcad38d4ebff2"
   },
   "source": [
    "### LSTM to Parse Strokes\n",
    "The model suggeted from the tutorial is\n",
    "\n",
    "![Suggested Model](https://www.tensorflow.org/versions/master/images/quickdraw_model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "f6d6f9011ea41c639a91c2b107575063f781574c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_2 (Batch (None, None, 3)           12        \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, None, 256)         4096      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, None, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, None, 256)         327936    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, None, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, None, 256)         327936    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, None, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, None, 256)         327936    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, None, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, None, 256)         196864    \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, None, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, None, 256)         196864    \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, None, 256)         0         \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, None, 256)         394240    \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, None, 256)         394240    \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, None, 256)         394240    \n",
      "_________________________________________________________________\n",
      "bidirectional_7 (Bidirection (None, None, 256)         394240    \n",
      "_________________________________________________________________\n",
      "bidirectional_8 (Bidirection (None, 256)               394240    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 340)               174420    \n",
      "=================================================================\n",
      "Total params: 3,658,848\n",
      "Trainable params: 3,658,842\n",
      "Non-trainable params: 6\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import BatchNormalization, Conv1D, LSTM, Dense, Dropout, Bidirectional\n",
    "#if len(get_available_gpus())>0:\n",
    "    # https://twitter.com/fchollet/status/918170264608817152?lang=en\n",
    "#    from keras.layers import CuDNNLSTM as LSTM # this one is about 3x faster on GPU instances\n",
    "stroke_read_model = Sequential()\n",
    "stroke_read_model.add(BatchNormalization(input_shape = (None,)+(3,)))\n",
    "# filter count and length are taken from the script https://github.com/tensorflow/models/blob/master/tutorials/rnn/quickdraw/train_model.py\n",
    "stroke_read_model.add(Conv1D(256, (5,), activation = 'relu'))\n",
    "stroke_read_model.add(Dropout(0.2))\n",
    "stroke_read_model.add(Conv1D(256, (5,), activation = 'relu'))\n",
    "stroke_read_model.add(Dropout(0.2))\n",
    "stroke_read_model.add(Conv1D(256, (5,), activation = 'relu'))\n",
    "stroke_read_model.add(Dropout(0.2))\n",
    "stroke_read_model.add(Conv1D(256, (5,), activation = 'relu'))\n",
    "stroke_read_model.add(Dropout(0.2))\n",
    "stroke_read_model.add(Conv1D(256, (3,), activation = 'relu'))\n",
    "stroke_read_model.add(Dropout(0.2))\n",
    "stroke_read_model.add(Conv1D(256, (3,), activation = 'relu'))\n",
    "stroke_read_model.add(Dropout(0.2))\n",
    "stroke_read_model.add(Bidirectional(LSTM(128, dropout = 0.3, recurrent_dropout= 0.3,  return_sequences = True)))\n",
    "stroke_read_model.add(Bidirectional(LSTM(128,dropout = 0.3, recurrent_dropout= 0.3, return_sequences = True)))\n",
    "stroke_read_model.add(Bidirectional(LSTM(128, dropout = 0.3, recurrent_dropout= 0.3,  return_sequences = True)))\n",
    "stroke_read_model.add(Bidirectional(LSTM(128,dropout = 0.3, recurrent_dropout= 0.3, return_sequences = True)))\n",
    "stroke_read_model.add(Bidirectional(LSTM(128,dropout = 0.3, recurrent_dropout= 0.3, return_sequences = False)))\n",
    "stroke_read_model.add(Dense(512, activation = 'relu'))\n",
    "stroke_read_model.add(Dropout(0.2))\n",
    "stroke_read_model.add(Dense(NCATS, activation = 'softmax'))\n",
    "stroke_read_model.compile(optimizer = 'adam', \n",
    "                          loss = 'categorical_crossentropy', \n",
    "                          metrics = ['categorical_accuracy', top_3_accuracy])\n",
    "stroke_read_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "2a549512-a9d9-4afd-b748-3e1c3296e193",
    "_uuid": "5fda10b30c47a8cf6ea822ed0a4a1d7cd2c81195"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/Keras-2.2.4-py3.6.egg/keras/callbacks.py:1062: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
     ]
    }
   ],
   "source": [
    "weight_path=\"{}_weights.best.hdf5\".format('stroke_lstm_bidirectional_relu')\n",
    "\n",
    "checkpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, \n",
    "                             save_best_only=True, mode='min', save_weights_only = True)\n",
    "\n",
    "\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=4, \n",
    "                                   verbose=1, mode='auto', epsilon=0.0001, cooldown=5, min_lr=0.0001)\n",
    "early = EarlyStopping(monitor=\"val_loss\", \n",
    "                      mode=\"min\", \n",
    "                      patience=3) \n",
    "callbacks_list = [checkpoint, early, reduceLROnPlat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "825b3af8-9451-487b-a1e1-538f2f1489e1",
    "_uuid": "ed2fc26af74aed1a93bbc253d61b72db5a81f5cc"
   },
   "outputs": [],
   "source": [
    "# Change the number of epochs to 20\n",
    "\n",
    "\n",
    "from IPython.display import clear_output\n",
    "hist = stroke_read_model.fit_generator(train_datagen, steps_per_epoch=STEPS, epochs=EPOCHS, verbose=1,\n",
    "                        validation_data=val_datagen, validation_steps = val_steps,\n",
    "                      callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "599c7c54e12353e6dcee3d6c6a29bc04064982d3"
   },
   "outputs": [],
   "source": [
    "hist_df = pd.DataFrame(hist.history) \n",
    "hist_df.to_csv('hist_training.csv')\n",
    "hist_df.index = np.arange(1, len(hist_df)+1)\n",
    "fig, axs = plt.subplots(nrows=2, sharex=True, figsize=(16, 10))\n",
    "axs[0].plot(hist_df.val_top_3_accuracy, lw=5, label='Validation Accuracy')\n",
    "axs[0].plot(hist_df.top_3_accuracy, lw=5, label='Training Accuracy')\n",
    "axs[0].set_ylabel('Accuracy')\n",
    "axs[0].set_xlabel('Epoch')\n",
    "axs[0].grid()\n",
    "axs[0].legend(loc=0)\n",
    "axs[1].plot(hist_df.val_loss, lw=5, label='Validation MLogLoss')\n",
    "axs[1].plot(hist_df.loss, lw=5, label='Training MLogLoss')\n",
    "axs[1].set_ylabel('MLogLoss')\n",
    "axs[1].set_xlabel('Epoch')\n",
    "axs[1].grid()\n",
    "axs[1].legend(loc=0)\n",
    "fig.savefig('hist.png', dpi=300)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a7eb5b62-cf57-4380-8786-9ddc05be658f",
    "_uuid": "858059b6c16d81f86460bef8fcf595e0d68d12b2"
   },
   "outputs": [],
   "source": [
    "valid_df = pd.read_csv(os.path.join(DP_DIR, 'train_k{}.csv.gz.gz'.format(NCSVS - 1)), nrows=34000)\n",
    "x_valid = df_to_image_array_xd(valid_df)\n",
    "y_valid = keras.utils.to_categorical(valid_df.y, num_classes=NCATS)\n",
    "lstm_results = stroke_read_model.evaluate(x_valid, y_valid, batch_size = 4096)\n",
    "print('Accuracy: %2.1f%%, Top 3 Accuracy %2.1f%%' % (100*lstm_results[1], 100*lstm_results[2]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e99b1ed154f26381d12918e2b4e12db807e6535f"
   },
   "source": [
    "# Submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "436a4fce-3843-4c84-8eeb-0161fe3c4e04",
    "_uuid": "4f3a40e23f2e917b68171822944491ab348e15b3"
   },
   "outputs": [],
   "source": [
    "sub_df = pd.read_csv(os.path.join(INPUT_DIR, 'test_simplified.csv'))\n",
    "sub_df['drawing'] = sub_df['drawing'].map(_stack_it)\n",
    "sub_vec = np.stack(sub_df['drawing'].values, 0)\n",
    "sub_pred = stroke_read_model.predict(sub_vec, verbose=True, batch_size=4096)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "72825ea87d35ad96b0254e3af5f5aaf64fb9c78f"
   },
   "outputs": [],
   "source": [
    "top3 = preds2catids(sub_pred)\n",
    "cats = list_all_categories()\n",
    "id2cat = {k: cat.replace(' ', '_') for k, cat in enumerate(cats)}\n",
    "top3cats = top3.replace(id2cat)\n",
    "sub_df['word'] = top3cats['a'] + ' ' + top3cats['b'] + ' ' + top3cats['c']\n",
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2b5ece83cb6095e95ef5741e73508d9129be1e3d"
   },
   "outputs": [],
   "source": [
    "sub_df[['key_id', 'word']].to_csv('lstm_relu_datagen.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "366a5a7b8bbf29317bb182d46bf8d48c730c440c"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
